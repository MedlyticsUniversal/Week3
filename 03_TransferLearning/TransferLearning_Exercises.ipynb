{"cells":[{"cell_type":"markdown","metadata":{"id":"2PYOnt_yJkaI","colab_type":"text","cell_id":"6da3b92285024a6abceae37039477c6e","deepnote_cell_type":"markdown"},"source":"# Transfer Learning Exercises","block_group":"00000-91035967-1f6d-4cda-a4fc-ae720c12bcf7"},{"cell_type":"code","metadata":{"id":"DNCmsEpLJkaJ","colab":{},"colab_type":"code","cell_id":"7c7b8afe1dd842f480b1c94d941c2cc4","deepnote_cell_type":"code"},"source":"# Import useful libraries (note: don't forget to turn on GPU)\n\n# Tensorflow for network building/training\nimport tensorflow as tf\nfrom tensorflow.python.keras import Model, Sequential\nfrom tensorflow.keras.applications import VGG16\n\n# Basic operating system (os), numerical, and plotting functionality\nimport os\nimport time\nimport numpy as np\nfrom matplotlib import pylab as plt\n\n# scikit-learn data utilities\nfrom sklearn.model_selection import train_test_split\nfrom skimage import transform\n\n# scikit-learn performance metric utilities\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n# Color transformations\nfrom skimage.color import rgb2lab\n\n# Skimage resizing \nfrom skimage.transform import resize\n\n# Garbage collection (for saving RAM during training)\nimport gc","block_group":"00001-2d03c604-2478-4fd5-b130-5e2ce13d4f13","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1IkUE60WJkaQ","colab_type":"text","cell_id":"3787b23a5d7349929e8f8e55e9fc01a8","deepnote_cell_type":"markdown"},"source":"## VGG16 Model\n\nFor this exercise, you'll now use the **VGG16** model as the feature extractor. https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16\n\nSpecifications:\n- Default input size: 224x224, no smaller than 32x32 pixels\n- Default output classes: 1000\n\nOur images are 150x150 pixels in size and come from only **8 categories**. In order to use this model for our classification task, we again can/need to do the following:\n* **Resize images**: Our input images can be resized to the appropriate dimensions. Alternatively, we can pad our images to the expected dimensions. Padding leads to additional choices. Do we pad with zeros, duplicate edge pixels, or mirror the image across edges?\n* **Change the prediction layer**: Remove the existing prediction layer and add a new layer that can predict 8 classes.\n* **Train**: Finally, we need to train the network on our data.","block_group":"00002-2a1aa0d4-6e94-4033-8aa2-fcea42d2566e"},{"cell_type":"markdown","metadata":{"id":"teogiXA3Jkak","colab_type":"text","cell_id":"fb42b61a8a8b4be38697b88fddcddb50","deepnote_cell_type":"markdown"},"source":"## Load Data","block_group":"00003-846be333-a2dd-49bf-8139-f7a97c1ce332"},{"cell_type":"markdown","metadata":{"id":"AwwkYk48Jkal","colab_type":"text","cell_id":"1016e9b29c9145e9af49fb1e68e9922f","deepnote_cell_type":"markdown"},"source":"Getting path and changing directories","block_group":"00004-01c2d56a-c4f8-452d-b4fa-9d223a481092"},{"cell_type":"code","metadata":{"id":"hYdSaY2PJkam","colab":{},"colab_type":"code","cell_id":"377cda755a494c8596e165ed5b0be03a","deepnote_cell_type":"code"},"source":"# Define the current directory and the directory where the files to download can be found\ncurrent_dir = os.getcwd()\nremote_path = 'https://raw.githubusercontent.com/MedlyticsUniversal/Data/main/Week3/nuclei_data/crc/'\n\n# Define and build a directory to save this data in\ndata_dir = os.path.join(current_dir, 'crc_data')\nif not os.path.isdir(data_dir):\n  os.mkdir(data_dir)\n\n# Move into the data directory and download all of the files\nos.chdir(data_dir)\nfor ii in range(1, 6):\n    basename = f'rgb0{ii}.npz'\n    filename = os.path.join(remote_path, basename)\n\n    # Check if the file has already been downloaded\n    if not os.path.isfile(basename):\n      cmd = f'wget {filename}'\n      print(cmd)\n      os.system(cmd)\n\n# Return to the original directory\nos.chdir(current_dir)","block_group":"00005-5ef20a32-3708-489b-81b2-fd256412e20e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NoYrP7JXJkat","colab_type":"text","cell_id":"815d06d8cfd241998c2ebc6df260d386","deepnote_cell_type":"markdown"},"source":"Function for loading images","block_group":"00006-6dbe7aae-4274-4a72-8827-02557d9e1ad1"},{"cell_type":"code","metadata":{"id":"1sesRgeVJkau","colab":{},"colab_type":"code","cell_id":"c294af07495d4bdca32cc9bfd3b89991","deepnote_cell_type":"code"},"source":"# Define a function to load the data from the assumed download path\ndef load_images(colorspace='rgb'):\n    \"\"\"\n    Loads the example data and applies transformation into requested colorspace\n\n    Arguments\n    ---------\n    colorspace : str, optional, default: `rgb`\n        The colorspace into which the images should be transformed. Accepted\n        values include\n\n        'rgb' : Standard red-green-blue color-space for digital images\n\n        'gray' or 'grey': An arithmetic average of the (r, g, b) values\n\n        'lab': The CIE L*a*b* colorspace\n    \n    Returns\n    -------\n    images : numpy.ndarray, shape (Nimg, Ny, Nx, Ncolor)\n        The complete set of transformed images\n\n    labels : numpy.ndarray, shape (Nimg)\n        The classification labels associated with each entry in `images`\n\n    label_to_str : dict\n        A dictionary which converts the numerical classification value in\n        `labels` into its string equivalent representation.\n    \"\"\"\n    # Check that the colorspace argument is recognized\n    colorspace_lower = colorspace.lower()\n    if colorspace_lower not in ['rgb', 'gray', 'grey', 'lab']:\n        raise ValueError(f'`colorspace` value of {colorspace} not recognized')\n\n    # Load data, which is stored as a numpy archive file (.npz)\n    filename = os.path.join(data_dir, 'rgb01.npz')\n    print(f'loading {filename}')\n    tmp = np.load(os.path.join(data_dir, 'rgb01.npz'), allow_pickle=True)\n\n    # Parse the loaded data into images and labels\n    # Initialize the images and labels variables using the first archive data\n    images = tmp['rgb_data']\n    if colorspace_lower == 'rgb':\n        pass\n    elif colorspace_lower in ['gray', 'grey']:\n        images = np.mean(images, axis=-1)      # Average into grayscale\n    elif colorspace_lower == 'lab':\n        images = rgb2lab(images)               # Convert to CIE L*a*b*\n\n    # Grab the initial array for the image labels\n    labels = tmp['labels']\n    \n    # Grab the dictionary to convert numerical labels to their string equivalent\n    label_to_str = tmp['label_str']\n    label_to_str = label_to_str.tolist() # Convert label_to_str into a dict\n\n    # Update the user on the number and size of images loaded\n    print('Loaded images with shape {}'.format(images.shape))\n    del tmp\n\n    # Loop over each of the remaining archives and append the contained data\n    for ii in range(2,6):\n        # Build the full path to the archive and load it into memory\n        filename = os.path.join(data_dir, f'rgb0{ii}.npz')\n        print(f'loading {filename}')\n        tmp = np.load(filename, allow_pickle=True)\n\n        # Parse and append the data\n        these_images = tmp['rgb_data']\n        if colorspace_lower == 'rgb':\n            pass\n        elif (colorspace_lower == 'gray') or (colorspace_lower == 'grey'):\n            these_images = np.mean(these_images, axis=-1) # Convert to grayscale\n        elif colorspace_lower == 'lab':\n            these_images = rgb2lab(these_images)          # Convert to CIEL*a*b*\n\n        # Append the images and labels\n        images = np.append(images, these_images, axis=0)\n        labels = np.append(labels, tmp['labels'], axis=0)\n\n        # Update the user on the number and size of images\n        print('Loaded images with shape {}'.format(these_images.shape))\n        del tmp\n\n    # Force the image data to be floating point and print the data shape\n    images = images.astype(float)\n    print('Final image data shape: {}'.format(images.shape))\n    print('Number of image labels: {}'.format(*labels.shape))\n\n    return images, labels, label_to_str","block_group":"00007-34e759b8-5a15-495c-9c20-634afa7f6e23","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2gg4B0EOJka1","colab_type":"text","cell_id":"a0451acb0acf4368bdb549a197fa5347","deepnote_cell_type":"markdown"},"source":"Load images and labels into memory","block_group":"00008-c5098aca-8733-4fd6-9aeb-040173f048f4"},{"cell_type":"code","metadata":{"id":"YXzdtPccJka3","colab":{},"colab_type":"code","cell_id":"cc2560001a764613b74e1318f5d7160d","deepnote_cell_type":"code"},"source":"images_full_res, labels, label_to_str = load_images()\nnum_classes = np.unique(labels).size","block_group":"00009-0b5975e4-ee31-48e6-a3b5-200a8c99f3a0","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U8ri4n4KK3Ql","colab_type":"text","cell_id":"d9680b1ff2604ed4a4f2813701d84193","deepnote_cell_type":"markdown"},"source":"## Pre-process the Images","block_group":"00010-e09d989c-d8ee-4424-adfa-194d1029743a"},{"cell_type":"markdown","metadata":{"id":"FZqvdtsqJka7","colab_type":"text","cell_id":"b7730eec3a2741e19e62ea401de7898b","deepnote_cell_type":"markdown"},"source":"Resizing\n> *Note: you'll have to edit a line of code in the cell for resizing*","block_group":"00011-18100c99-b5c8-4985-8216-73b0f981b106"},{"cell_type":"code","metadata":{"id":"6gAiiGUSJka7","colab":{},"colab_type":"code","cell_id":"e0909edebb784c199e6765f8ccea0ee2","deepnote_cell_type":"code"},"source":"# This boolean can be switched to false if you do not want to resize the images\nresize_images_bool = True\n\n# Specify a new shape to use for the resized images\n# NOTE: For the VGG16 model, we must use a size of at least (32, 32).\noriginal_shape = images_full_res.shape\nnew_shape = list(original_shape)\nnew_shape[1:3] = ## YOUR CODE HERE\n\n# Compute if we are downsampling (in which case we need anti-aliasing)\nscaling_ratio = np.array(new_shape[1:3])/np.array(original_shape[1:3])\nanti_alias = np.any(scaling_ratio < 1)\n\n# If resizing is requested, then run the resizing transformation\nif resize_images_bool:\n    # Grab the original shape of the images\n    num_images = images_full_res.shape[0]\n\n    # Initialize an array for storing the resized images\n    images = np.zeros(new_shape, dtype=np.float16)\n\n    # Loop over each image in the data and perform a resizing operation\n    for img_num, img_data in enumerate(images_full_res):\n        # Update the user on progress\n        if np.mod(img_num, 1000) == 0:\n            print(f'Processing image number {img_num}')\n\n        # Process the image and force it to be a 16-bit float\n        processed_img = transform.resize(img_data, new_shape[1:],\n                                         anti_aliasing=anti_alias)\n        images[img_num] = processed_img.astype(np.float16)\n\n# If no resizing requested, then just rename that data\nelse:\n    images = images_full_res\n\n# Remove the full-resolution versions from memory (just clogging things up)\ndel images_full_res","block_group":"00012-ba4b0501-b1e6-4349-8dc0-de1f8339a0ce","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xQjuP1H1JkbE","colab_type":"text","cell_id":"81d8dc576eb6439887366ada1a58e5b7","deepnote_cell_type":"markdown"},"source":"Normalize the images (if it hasn't been done already)","block_group":"00013-c841dc55-01a8-402c-a7ec-7640fbe1b36e"},{"cell_type":"code","metadata":{"id":"ZE3wO4fKJkbF","colab":{},"colab_type":"code","cell_id":"b362094078dc43f196fe524376145592","deepnote_cell_type":"code"},"source":"# Note, we cast image data as float16 to save RAM\nimages = images.astype(np.float16)/255.0","block_group":"00014-cf06a3df-577c-4ced-bd0c-c37e1042ded3","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dA8WJ25FTKjS","colab_type":"text","cell_id":"2a6b733b6a2e4a389906b47d6c9ca1a8","deepnote_cell_type":"markdown"},"source":"Include an axis for color channels","block_group":"00015-91f62a92-07f0-4098-a36a-fa75f241577e"},{"cell_type":"code","metadata":{"id":"bNYIGee0TNK1","colab":{},"colab_type":"code","cell_id":"d175bc24392c4dfb952283b0a7ee49a7","deepnote_cell_type":"code"},"source":"# Take note of number of color channels in the loaded image add a last axis to \n# images ndarray if array dimension is only 3 (as is the case with grayscale images)\nif images.ndim == 3:\n    # If image is grayscale, then we add a last axis (of len 1) for channel\n    n_channels = 1\n    images = images[:, : , :, np.newaxis]\n    print('\\nlast dimension added to images ndarray to account for channel')\n    print(f'new images.shape: {images.shape}')\nelse:\n    #if image is not grayscale, last dimension of image already corresponds to channel\n    n_channels = images.shape[-1]","block_group":"00016-8056a91a-1fe7-41a7-bcc1-e649858b21af","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rHV6NK27JkbA","colab_type":"text","cell_id":"07909d0b3e0c4eb8bf0c29811e793c0d","deepnote_cell_type":"markdown"},"source":"Split data into train and test sets","block_group":"00017-d548fabb-16e1-4ff8-b110-9cb1c89a1bcc"},{"cell_type":"code","metadata":{"id":"Fwmn7EuuJkbA","colab":{},"colab_type":"code","cell_id":"2f8f11975d1b406385a009164adb0d76","deepnote_cell_type":"code"},"source":"# Split data into train and test sets\ntrain_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=.2)\n\n# Convert 'labels' (1D array of integers) to one-hot encoding\ntrain_labels = tf.keras.utils.to_categorical(train_labels)\ntest_labels = tf.keras.utils.to_categorical(test_labels)\n\n# Print sizes of train/test sets\nprint(f'train_images.shape: {train_images.shape}')\nprint(f'train_labels.shape: {train_labels.shape}')\nprint(f'test_images.shape: {test_images.shape}')\nprint(f'test_labels.shape: {test_labels.shape}')\n\n# Print the one-hot encoded labels as a sanity check\nprint('one-hot encoded labels:')\nprint(train_labels)\n\n# Get rid of the duplicate copies of the data\ndel images, labels","block_group":"00018-448c791b-8866-4304-b690-6c5390e6aefd","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4D9uhLlSJkbM","colab_type":"text","cell_id":"601f746dff3c415a84e5af8135695044","deepnote_cell_type":"markdown"},"source":"## Load Pre-trained VGG16 Model\n\nHere's the link to the documentation: https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16.","block_group":"00019-2bac7120-ec9e-4219-be0a-be17493f9431"},{"cell_type":"code","metadata":{"id":"Om5GOu21JkbO","colab":{},"colab_type":"code","cell_id":"287d071a1bef4fd0b2a46c60a88878ab","deepnote_cell_type":"code"},"source":"# Create the base pre-trained model\nprint('loading VGG16')\nbase_model = ## YOUR CODE HERE\nprint('done')","block_group":"00020-28fd41fe-f615-4f1e-bee4-a1fe2c108083","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RhA1SR6VJkbR","colab_type":"text","cell_id":"dd9e0bf2190948858c4e705463cf332c","deepnote_cell_type":"markdown"},"source":"Summarize model structure","block_group":"00021-845a59b4-ce65-4020-b26c-7576687285dc"},{"cell_type":"code","metadata":{"id":"5sQGrHQjJkbR","colab":{},"colab_type":"code","cell_id":"9f0df862f9bb4094adc6272e1c17aae1","deepnote_cell_type":"code"},"source":"base_model.summary()","block_group":"00022-f5b7f47a-f81e-49f3-b9ea-15981948640f","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zYNFl2ZOJkbZ","colab_type":"text","cell_id":"29eea3704beb4d1cb1b70605d361cb00","deepnote_cell_type":"markdown"},"source":"Freezing layers","block_group":"00023-37cbf075-b9a6-4b9d-be9c-158353bd2f2b"},{"cell_type":"code","metadata":{"id":"oLC3uT3cJkba","colab":{},"colab_type":"code","cell_id":"847a453558324c5a9fd7c8890b7c5b9f","deepnote_cell_type":"code"},"source":"# Play around with freezing layers, take a look at the tutorial notebook for reference \n\n# By default we'll just freeze the entire base model again\nbase_model.trainable = False","block_group":"00024-7ec76657-decf-4ece-83c9-b97649d5a01a","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wjn-7TTxJkbV","colab_type":"text","cell_id":"0b0dda516ec04ce18fdf2b09460129ac","deepnote_cell_type":"markdown"},"source":"Modify the pre-trained network by adding a few new layers at the output, including a classification layer (remember we want to predict 8 different classes)","block_group":"00025-5d246049-9efb-4ebb-9276-5c0563759d5e"},{"cell_type":"code","metadata":{"id":"-skRWsOAJkbW","colab":{},"colab_type":"code","cell_id":"ba534f2926174e608f930015b6339962","deepnote_cell_type":"code"},"source":"# Add a global spatial average pooling layer\n## YOUR CODE HERE\n\n# Add a fully-connected layer\n## YOUR CODE HERE\n\n# Add the final classification layer\n## YOUR CODE HERE\n\n# Build the model you will train\nmodel = ## YOUR CODE HERE\n\n# Print summary of model layers\nmodel.summary()","block_group":"00026-17059d16-ad67-455a-ab70-edfa1c8f942f","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yvwvRkiVJkbd","colab_type":"text","cell_id":"1ccc2844e1944e64bc7c38c74790b018","deepnote_cell_type":"markdown"},"source":"Compiling model","block_group":"00027-17137033-c484-4eeb-95af-c3e44c275fcc"},{"cell_type":"code","metadata":{"id":"tC-_ZFi_Jkbe","colab":{},"colab_type":"code","cell_id":"eee2cb4cfee84d9aacd60bbb29b0717c","deepnote_cell_type":"code"},"source":"# Compile the model (should be done *after* setting layers to non-trainable)\n    # optimizer: rmsprop\n    # loss: categorical crossentropy\n    # metrics: accuracy\n  \n## YOUR CODE HERE","block_group":"00028-5ed1b7a4-b3d4-40f8-ba44-deed0f922007","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"29DxiSHbJkbh","colab_type":"text","cell_id":"0886c65e73c24c0199654ae64edb81f5","deepnote_cell_type":"markdown"},"source":"## Train model","block_group":"00029-32c95200-2015-473d-a1ce-f186216facea"},{"cell_type":"markdown","metadata":{"id":"XsYo9vGeeMBM","colab_type":"text","cell_id":"914045b149434f1bb6e199eb1a022716","deepnote_cell_type":"markdown"},"source":"Train the model on the new histological data","block_group":"00030-8878fece-55d7-45f8-9068-0ad1fb8f9431"},{"cell_type":"code","metadata":{"id":"GXtYUBTOeWl6","colab":{},"colab_type":"code","cell_id":"76f3917cec814dd990de6b45ae817cb1","deepnote_cell_type":"code"},"source":"# Convert all of our training and validation ('test') data to TensorFlow data\n# This prevents the training algorithm from needing to make a *copy* of your\n# numpy arrays, which would EAT UP SOO MUCH RAM!\n\n# It also accelerates training a bit because there is no data-conversion step\ntrain_images_tf = tf.constant(train_images, dtype=tf.float16)\ntest_images_tf = tf.constant(test_images)\ndel train_images, test_images\n\ntrain_labels_tf = tf.constant(train_labels, dtype=tf.float16)\ntest_labels_tf = tf.constant(test_labels)\ndel train_labels, test_labels","block_group":"00031-e8922a04-2c26-4910-8516-fa95a9d79bbb","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2J-uBwdWerN5","colab_type":"text","cell_id":"85f793fa48744bbb95e45bed6e4b6873","deepnote_cell_type":"markdown"},"source":"Train model","block_group":"00032-1858e45f-fb35-453c-b5ef-7350a22c507a"},{"cell_type":"code","metadata":{"id":"vpKK3afvJkbi","colab":{},"colab_type":"code","cell_id":"ba527ce6e540433d957812dc068ae107","deepnote_cell_type":"code"},"source":"# This function is called after each epoch\n# (It will ensure that your training process does not consume all available RAM)\nclass garbage_collect_callback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs=None):\n    gc.collect()\n\n# Time how long it takes the model to train for these epochs\nstart_time = time.time()\n\n# Perform the training method\nhistory = model.fit(train_images_tf,\n                    train_labels_tf,\n                    batch_size=64,\n                    epochs= 50,\n                    verbose=1,\n                    validation_data=(test_images_tf, test_labels_tf),\n                    callbacks = [garbage_collect_callback()])\n\nstop_time = time.time()\nprint(\"--- %s seconds ---\" % (stop_time - start_time))","block_group":"00033-c005cb49-8516-41aa-b63e-192dd9daf572","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wPGyseHcJkbk","colab_type":"text","cell_id":"8703fe26b201427f9a81b86c5c16a926","deepnote_cell_type":"markdown"},"source":"Plot model's train/validation accuracy and loss","block_group":"00034-b58012d7-9e62-4371-9360-ee69852d64dd"},{"cell_type":"code","metadata":{"id":"EXegpORpJkbl","colab":{},"colab_type":"code","cell_id":"c99f98e7df194fada5e1fbb13455861f","deepnote_cell_type":"code"},"source":"print(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","block_group":"00035-cd47517c-d824-49a7-938c-a24095a5c40d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n8GR-I4mJkbp","colab_type":"text","cell_id":"737af653d00443518b0d2b35e4d93aae","deepnote_cell_type":"markdown"},"source":"## Make Predictions for Test Images","block_group":"00036-7267320e-67c6-412f-8f16-d9c5825ca890"},{"cell_type":"code","metadata":{"id":"QucAFGw1Jkbq","colab":{},"colab_type":"code","cell_id":"ba0fa61da40b4f03ae277d22347b869b","deepnote_cell_type":"code"},"source":"# Predict class of test each test\npredictions = model.predict(test_images_tf, verbose=True)\n\n# Convert the predictions and true labels into category numbers\ntest_true_labels = test_labels_tf.numpy().argmax(axis=1)\ntest_pred_labels = predictions.argmax(axis=1)","block_group":"00037-0006f603-82be-43f2-a530-f542081927c7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rHx3kt6GJkbs","colab":{},"colab_type":"code","cell_id":"e3818afa825d45efbeeca4fefe6837c2","deepnote_cell_type":"code"},"source":"# Plot a set of test images, along with predicted labels and true labels\nplt.figure(figsize=(16,20))\nfor ii in range(0, 16):\n    # Activate subplot and display image\n    plt.subplot(4,4,ii+1)\n    plt.imshow(test_images_tf[ii+100,:,:,:].numpy().astype(float))\n\n    # Turn off axes\n    plt.axis('off')\n\n    # Add annotaiton\n    plt.title('expected : ' + label_to_str[test_true_labels[ii+100]]\n              + '\\npredicted : ' + label_to_str[test_pred_labels[ii+100]])\nplt.show()","block_group":"00038-79eb9e20-a1db-4b41-9b6a-67780e376c7b","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ntDR6ZWYiEgN","colab_type":"text","cell_id":"1caf4b8585da41368f0b6fc423c5b61d","deepnote_cell_type":"markdown"},"source":"## Accuracy","block_group":"00039-d0e7da00-f8a6-4c98-a62e-dafa30366092"},{"cell_type":"code","metadata":{"id":"yRrEEXx9iGZK","colab":{},"colab_type":"code","cell_id":"0fe0f3040e7c44faa471b3c681fca6ca","deepnote_cell_type":"code"},"source":"acc = accuracy_score(test_true_labels, test_pred_labels)\nprint(f'Model Accuracy: {acc:.2%}')","block_group":"00040-afb6a05c-929b-4e48-9b7f-3e1deb3ac9b4","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hpj8PtZuiIAU","colab_type":"text","cell_id":"fea37ab8a0f34568956fef8f8802ca31","deepnote_cell_type":"markdown"},"source":"Confusion matrix","block_group":"00041-9077ddd7-488f-404a-826d-90210bff1097"},{"cell_type":"code","metadata":{"id":"pp7rdgkciJJJ","colab":{},"colab_type":"code","cell_id":"4be81b5ad4364499bdbc04e065ab05f5","deepnote_cell_type":"code"},"source":"conf_mat = confusion_matrix(test_true_labels, test_pred_labels)\n\n# Generate a new figure\nplt.figure(figsize=(10,10))\n\n# Display the confusion matrix\nplt.imshow(conf_mat, cmap='hot', interpolation='nearest')\n\n# Add some anotation for the plot\nplt.colorbar()\nplt.xlabel('True label')\nplt.ylabel('Predicted label')\nplt.show()","block_group":"00042-601202e6-e80a-4f13-a3cd-4fd037bebb15","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XFwpTudBiMdv","colab_type":"text","cell_id":"3d06720d3d644202abe238a8cfa27182","deepnote_cell_type":"markdown"},"source":"### To-do:\n\nContinue playing around with preprocessing (image size) and the model (added layers, freezing layers, optimizer, # epochs) and see their effects on the accuracy. Doing this may help you for the challenge problem :O","block_group":"00043-033f3a41-d186-4290-a61c-67a0c6ea209c"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=7e57fe3a-b00b-4f56-b5f4-11f8b28e636f' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03_TransferLearning_Exercises.ipynb","provenance":[]},"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"deepnote_notebook_id":"88ab00edf5724911b678c8ad0826b38e","deepnote_execution_queue":[]}}